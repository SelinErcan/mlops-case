{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to an error with the Win32api module. Consider (re) installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresWin32Api'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import zscore\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "from sklearn.preprocessing import  OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"localhost\"  \n",
    "port = \"5432\"\n",
    "dbname = \"loan_database\"  \n",
    "user = \"myuser\" \n",
    "password = \"mypassword\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=port,\n",
    "    dbname=dbname,\n",
    "    user=user,\n",
    "    password=password\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SELECT * FROM loan_data;\")\n",
    "\n",
    "results = cur.fetchall()\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\n",
    "    \"person_age\", \"person_gender\", \"person_education\", \"person_income\",\n",
    "    \"person_emp_exp\", \"person_home_ownership\", \"loan_amnt\", \"loan_intent\",\n",
    "    \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\", \n",
    "    \"credit_score\", \"previous_loan_defaults_on_file\", \"loan_status\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"person_age\"] = df[\"person_age\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name, dtype in df.dtypes.items():\n",
    "    if dtype == \"object\":\n",
    "        print(f\"Column Name: {col_name} - Distinct values: {df[col_name].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subplots(columns, col_type, num_col = 4):\n",
    "    num_plots = len(columns)\n",
    "    num_rows = (num_plots // num_col) + (num_plots % num_col > 0) \n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_col, figsize=(num_col * 8, num_rows * 5))\n",
    "\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col_name in enumerate(columns):\n",
    "        if col_type==\"number\":\n",
    "            sns.histplot(df[col_name], kde=True, bins=30, ax=axes[i])\n",
    "            axes[i].set_title(col_name)\n",
    "        if col_type == \"category\":\n",
    "            df[col_name].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, ax=axes[i])\n",
    "            axes[i].set_title(f\"Pie Chart of {col_name}\")\n",
    "            axes[i].set_ylabel(\"\")  # Remove default y-label\n",
    "\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=np.number).columns\n",
    "create_subplots(columns = numeric_columns, col_type = \"number\", num_col = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "create_subplots(columns = categorical_columns, col_type = \"category\", num_col = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=[float, int])\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f', cbar=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"person_age\", \"person_emp_exp\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [col for col in numeric_columns if col not in [\"person_age\", \"person_emp_exp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop('loan_status', axis=1)\n",
    "y = df['loan_status']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [col for col in numeric_columns if col not in [\"loan_status\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = X.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = df.columns.tolist()  \n",
    "column_list_str = ', '.join(column_list)  \n",
    "\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS training_data (\n",
    "    person_gender VARCHAR(50),\n",
    "    person_education VARCHAR(50),\n",
    "    person_income FLOAT,\n",
    "    person_home_ownership VARCHAR(50),\n",
    "    loan_amnt FLOAT,  \n",
    "    loan_intent VARCHAR(50),\n",
    "    loan_int_rate FLOAT,  \n",
    "    loan_percent_income FLOAT,\n",
    "    cb_person_cred_hist_length FLOAT,\n",
    "    credit_score INT,\n",
    "    previous_loan_defaults_on_file VARCHAR(10),\n",
    "    loan_status FLOAT,\n",
    "    CONSTRAINT unique_training_columns UNIQUE (person_gender, person_education, person_income, person_home_ownership, \n",
    "        loan_amnt, loan_intent, loan_int_rate, loan_percent_income, cb_person_cred_hist_length, \n",
    "        credit_score, previous_loan_defaults_on_file) \n",
    ");\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "\n",
    "train_data = X_train.copy() \n",
    "train_data['loan_status'] = y_train  \n",
    "\n",
    "data_to_insert = train_data.values.tolist()\n",
    "\n",
    "insert_query = f\"\"\"\n",
    "    INSERT INTO training_data ({column_list_str})\n",
    "    VALUES %s\n",
    "    ON CONFLICT ON CONSTRAINT unique_training_columns \n",
    "    DO NOTHING; \n",
    "\"\"\"\n",
    "\n",
    "execute_values(cur, insert_query, data_to_insert)\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = X_test.copy()\n",
    "test_data['loan_status'] = y_test\n",
    "test_data.to_csv('../data/test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler(quantile_range=(20, 80))\n",
    "\n",
    "X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n",
    "X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "encoded_train_categories = encoder.fit_transform(X_train[categorical_columns])\n",
    "encoded_test_categories = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "encoded_train_df = pd.DataFrame(encoded_train_categories, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "encoded_test_df = pd.DataFrame(encoded_test_categories, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "X_train_encoded = pd.concat([X_train[numeric_columns].reset_index(drop=True), encoded_train_df.reset_index(drop=True)], axis=1)\n",
    "X_test_encoded = pd.concat([X_test[numeric_columns].reset_index(drop=True), encoded_test_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_train_encoded = X_train_encoded.values\n",
    "X_test_encoded = X_test_encoded.values\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"X_train_encoded shape: {X_train_encoded.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test_encoded shape: {X_test_encoded.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = np.abs(zscore(X_train_encoded))\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "mask = (z_scores < threshold).all(axis=1)  \n",
    "X_train_encoded = X_train_encoded[mask]\n",
    "y_train = y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=\"relu\", input_shape=(X_train_encoded.shape[1],)),  \n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\")  \n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=build_model, epochs=10, batch_size=10, verbose=0)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = cross_val_score(estimator, X_train_encoded, y_train, cv=stratified_kfold, n_jobs=2)\n",
    "print(f\"Cross-Validation Mean Accuracy: {cv_results.mean():.4f} ± {cv_results.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psi(expected, observed, buckets=10):\n",
    "    \"\"\"\n",
    "    Calculate PSI between two distributions.\n",
    "    \"\"\"\n",
    "    expected_bins = pd.cut(expected, bins=buckets, include_lowest=True)\n",
    "    observed_bins = pd.cut(observed, bins=expected_bins.categories, include_lowest=True)\n",
    "    \n",
    "    expected_counts = expected_bins.value_counts()\n",
    "    observed_counts = observed_bins.value_counts()\n",
    "    \n",
    "    expected_pct = expected_counts / len(expected)\n",
    "    observed_pct = observed_counts / len(observed)\n",
    "    \n",
    "    all_bins = expected_bins.categories\n",
    "    expected_pct = expected_pct.reindex(all_bins, fill_value=0)\n",
    "    observed_pct = observed_pct.reindex(all_bins, fill_value=0)\n",
    "\n",
    "    psi_value = np.sum((observed_pct - expected_pct) * np.log(observed_pct / expected_pct))\n",
    "    \n",
    "    return psi_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psi(train_data, test_data, bins=10):\n",
    "    min_val = min(np.min(train_data), np.min(test_data))\n",
    "    max_val = max(np.max(train_data), np.max(test_data))\n",
    "    bin_edges = np.linspace(min_val, max_val, bins+1)\n",
    "\n",
    "    train_counts, _ = np.histogram(train_data, bins=bin_edges)\n",
    "    test_counts, _ = np.histogram(test_data, bins=bin_edges)\n",
    "\n",
    "    train_proportions = train_counts / len(train_data)\n",
    "    test_proportions = test_counts / len(test_data)\n",
    "\n",
    "    epsilon = 1e-8\n",
    "    psi = np.sum((train_proportions - test_proportions) * np.log((train_proportions + epsilon) / (test_proportions + epsilon)))\n",
    "    \n",
    "    return psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_density_stats(data):\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    skewness = stats.skew(data)\n",
    "    kurtosis = stats.kurtosis(data)\n",
    "    \n",
    "    return mean, std_dev, skewness, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_analysis_dict = {}\n",
    "\n",
    "for i in range(len(train_columns)):\n",
    "    train_data = X_train_encoded[:, i]\n",
    "    test_data = X_test_encoded[:, i]\n",
    "    \n",
    "    psi = calculate_psi(train_data, test_data)\n",
    "    mean_train, std_dev_train, skewness_train, kurtosis_train = calculate_density_stats(train_data)\n",
    "    mean_test, std_dev_test, skewness_test, kurtosis_test = calculate_density_stats(test_data)\n",
    "    \n",
    "    feature_key = f\"{train_columns[i]}\"\n",
    "    dist_analysis_dict[feature_key] = {\n",
    "        'PSI': psi,\n",
    "        'Train_Mean': mean_train,\n",
    "        'Train_Std_Dev': std_dev_train,\n",
    "        'Train_Skewness': skewness_train,\n",
    "        'Train_Kurtosis': kurtosis_train,\n",
    "        'Test_Mean': mean_test,\n",
    "        'Test_Std_Dev': std_dev_test,\n",
    "        'Test_Skewness': skewness_test,\n",
    "        'Test_Kurtosis': kurtosis_test\n",
    "    }\n",
    "\n",
    "print(\"\\nPSI Values Between Training and Test Data:\")\n",
    "for feature, stats in dist_analysis_dict.items():\n",
    "    psi = stats['PSI']\n",
    "    message = \"No significant shift\" if psi < 0.1 else \"Minor shift\" if psi < 0.25 else \"Significant shift\"\n",
    "    \n",
    "    print(f\"{feature}: PSI = {psi:.4f} -> {message}\")\n",
    "    print(f\"  Train Stats -> Mean: {stats['Train_Mean']:.4f}, Std Dev: {stats['Train_Std_Dev']:.4f}, \"\n",
    "          f\"Skewness: {stats['Train_Skewness']:.4f}, Kurtosis: {stats['Train_Kurtosis']:.4f}\")\n",
    "    print(f\"  Test Stats  -> Mean: {stats['Test_Mean']:.4f}, Std Dev: {stats['Test_Std_Dev']:.4f}, \"\n",
    "          f\"Skewness: {stats['Test_Skewness']:.4f}, Kurtosis: {stats['Test_Kurtosis']:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = build_model()\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "final_model.fit(X_train_encoded, y_train, epochs=20, batch_size=132, validation_split=0.2, callbacks=[reduce_lr])\n",
    "\n",
    "y_train_pred_prob = final_model.predict(X_train_encoded)\n",
    "y_train_pred = (y_train_pred_prob > 0.5).astype(int)  \n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "precision_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"\\nModel Evaluation on Training Set:\")\n",
    "print(f\"Accuracy:  {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall:    {recall_train:.4f}\")\n",
    "print(f\"F1-score:  {f1_train:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../fastapi/model/\"):\n",
    "    os.makedirs(\"../fastapi/model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Loan Prediction Retraining\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    mlflow.keras.log_model(final_model, \"model\")\n",
    "        \n",
    "    result = mlflow.register_model(\"runs:/{}/model\".format(mlflow.active_run().info.run_id), \"LoanPredictionModel\")\n",
    "        \n",
    "    mlflow.log_metric(\"accuracy\", accuracy_train)\n",
    "    mlflow.log_metric(\"precision\", precision_train)\n",
    "    mlflow.log_metric(\"recall\", recall_train)\n",
    "    mlflow.log_metric(\"f1_score\", f1_train)\n",
    "    mlflow.log_artifact(\"model/scaler.pkl\")\n",
    "    mlflow.log_artifact(\"model/encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../fastapi/model/scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "with open('../fastapi/model/encoder.pkl', 'wb') as encoder_file:\n",
    "    pickle.dump(encoder, encoder_file)\n",
    "\n",
    "final_model.save(\"../fastapi/model/model.h5\")\n",
    "\n",
    "print(\"Model files are saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
